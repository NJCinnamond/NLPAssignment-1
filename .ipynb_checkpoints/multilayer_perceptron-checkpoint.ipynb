{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import sys\n",
    "from util import evaluate, load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, train_data, dev_data, test_data, label_to_index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df (pandas.DataFrame): the dataset\n",
    "        \"\"\"\n",
    "        self.train_data = train_data\n",
    "        self.train_size = len(self.train_data)\n",
    "\n",
    "        self.dev_data = dev_data\n",
    "        self.validation_size = len(self.dev_data)\n",
    "\n",
    "        self.test_data = test_data\n",
    "        self.test_data = len(self.test_data)\n",
    "        \n",
    "        self.label_to_index = label_to_index\n",
    "        self.index_to_label = {v: k for k, v in label_to_index.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "    \n",
    "    def set_split(self, split):\n",
    "        if split == 'train':\n",
    "            self._target = self.train_data\n",
    "            self._target_size = len(self.train_data)\n",
    "        elif split == 'dev':\n",
    "            self._target = self.dev_data\n",
    "            self._target_size = len(self.dev_data)\n",
    "        elif split == 'test':\n",
    "            self._target = self.test_data\n",
    "            self._target_size = len(self.test_data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"the primary entry point method for PyTorch datasets\n",
    "        \n",
    "        Args:\n",
    "            index (int): the index to the data point \n",
    "        Returns:\n",
    "            a dictionary holding the data point's:\n",
    "                features (x_data)\n",
    "                label (y_target)\n",
    "                feature length (x_length)\n",
    "        \"\"\"\n",
    "        row = self._target[index]\n",
    "        \n",
    "        x_data = row[0]\n",
    "        y_target = [0] * len(self.label_to_index.keys())\n",
    "        y_target[self.label_to_index[row[1]]] = 1\n",
    "        \n",
    "        return {'x_data': torch.Tensor(x_data), \n",
    "                'y_target': torch.Tensor(y_target).to(device='cpu', dtype=torch.int64)}\n",
    "\n",
    "    def get_num_batches(self, batch_size):\n",
    "        \"\"\"Given a batch size, return the number of batches in the dataset\n",
    "        \n",
    "        Args:\n",
    "            batch_size (int)\n",
    "        Returns:\n",
    "            number of batches in the dataset\n",
    "        \"\"\"\n",
    "        return len(self) // batch_size\n",
    "\n",
    "    \n",
    "def generate_batches(dataset, batch_size, shuffle=True,\n",
    "                     drop_last=True, device=\"cpu\"): \n",
    "    \"\"\"\n",
    "    A generator function which wraps the PyTorch DataLoader. It will \n",
    "      ensure each tensor is on the write device location.\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, batch_size=1):\n",
    "        super(MultiLayerPerceptron, self).__init__()\n",
    "                \n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x_in):\n",
    "        out = self.fc1(x_in)\n",
    "        out = self.fc2(out)\n",
    "        scores = F.softmax(out, dim=1)\n",
    "        \n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dummy_bias(data):\n",
    "    for sample in data:\n",
    "        sample[0].append(1)\n",
    "    return data \n",
    "\n",
    "train_data, dev_data, test_data, data_type, label_to_index = load_data(['propername'])\n",
    "\n",
    "train_data = create_dummy_bias(train_data)\n",
    "dev_data = create_dummy_bias(dev_data)\n",
    "test_data = create_dummy_bias(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(train_data, dev_data, test_data, label_to_index)\n",
    "\n",
    "input_dim = len(train_data[0][0])\n",
    "hidden_dim = 200\n",
    "output_dim = len(label_to_index)\n",
    "\n",
    "classifier = MultiLayerPerceptron(input_dim, hidden_dim, output_dim, label_to_index)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=0.01)\n",
    "\n",
    "def compute_accuracy(y_pred, y_target):\n",
    "    _, y_pred_indices = y_pred.max(dim=1)\n",
    "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "    return n_correct / len(y_pred_indices) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss : 1.3524676378829272\n",
      "Training acc:  69.05709342560559\n",
      "Development acc:  67.37024221453284\n",
      "Loss : 1.3432883392877653\n",
      "Training acc:  70.00865051903108\n",
      "Development acc:  70.65743944636678\n",
      "Loss : 1.3190839952148\n",
      "Training acc:  72.4264705882353\n",
      "Development acc:  71.59169550173011\n",
      "Loss : 1.3071663751424785\n",
      "Training acc:  73.64619377162634\n",
      "Development acc:  71.34948096885815\n",
      "Loss : 1.3185158951987819\n",
      "Training acc:  72.49999999999984\n",
      "Development acc:  72.31833910034608\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-9e80f50a8b8e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;31m# step 5. use optimizer to take gradient step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m         \u001b[1;31m# -----------------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;31m# compute the accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m     92\u001b[0m                 \u001b[1;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m                 \u001b[0mexp_avg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m                     \u001b[1;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#TODO:\n",
    "#HYPERPARAMETER SELECTION ON NETWORK WITH FEW LAYERS\n",
    "#THEN RUN TESTS ON DEEPER NETWORK\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    dataset.set_split('train')\n",
    "    batch_generator = generate_batches(dataset, batch_size=10)\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    classifier.train()\n",
    "        \n",
    "    for batch_index, batch_dict in enumerate(batch_generator):\n",
    "        # the training routine is these 5 steps:\n",
    "\n",
    "        # --------------------------------------\n",
    "        # step 1. zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # step 2. compute the output\n",
    "        y_pred = classifier(batch_dict['x_data'])\n",
    "\n",
    "        # step 3. compute the loss\n",
    "        loss = loss_func(y_pred, torch.max(batch_dict['y_target'],1)[1])\n",
    "        loss_t = loss.item()\n",
    "        running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "        # step 4. use loss to produce gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # step 5. use optimizer to take gradient step\n",
    "        optimizer.step()\n",
    "        # -----------------------------------------\n",
    "        # compute the accuracy\n",
    "        acc_t = compute_accuracy(y_pred, torch.max(batch_dict['y_target'],1)[1])\n",
    "        running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "    print(\"Loss :\", running_loss)\n",
    "    print(\"Training acc: \", running_acc)\n",
    "    \n",
    "    # setup: batch generator, set loss and acc to 0; set eval mode on\n",
    "    dataset.set_split('dev')\n",
    "    batch_generator = generate_batches(dataset, batch_size=10)\n",
    "    \n",
    "    running_loss = 0.\n",
    "    running_acc = 0.\n",
    "    classifier.eval()\n",
    "        \n",
    "    for batch_index, batch_dict in enumerate(batch_generator):\n",
    "\n",
    "        # compute the output\n",
    "        y_pred =  classifier(batch_dict['x_data'])\n",
    "\n",
    "        # step 3. compute the loss\n",
    "        loss = loss_func(y_pred, torch.max(batch_dict['y_target'],1)[1])\n",
    "        loss_t = loss.item()\n",
    "        running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "        # compute the accuracy\n",
    "        acc_t = compute_accuracy(y_pred, torch.max(batch_dict['y_target'],1)[1])\n",
    "        running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "        \n",
    "    print(\"Development acc: \", running_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
