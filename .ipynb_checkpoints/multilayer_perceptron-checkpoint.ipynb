{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import sys\n",
    "from util import evaluate, load_data\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, train_data, dev_data, test_data, label_to_index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df (pandas.DataFrame): the dataset\n",
    "        \"\"\"\n",
    "        self.train_data = train_data\n",
    "        self.train_size = len(self.train_data)\n",
    "\n",
    "        self.dev_data = dev_data\n",
    "        self.validation_size = len(self.dev_data)\n",
    "\n",
    "        self.test_data = test_data\n",
    "        self.test_size = len(self.test_data)\n",
    "        \n",
    "        self.label_to_index = label_to_index\n",
    "        self.index_to_label = {v: k for k, v in label_to_index.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "    \n",
    "    def set_split(self, split):\n",
    "        if split == 'train':\n",
    "            self._target = self.train_data\n",
    "            self._target_size = len(self.train_data)\n",
    "        elif split == 'dev':\n",
    "            self._target = self.dev_data\n",
    "            self._target_size = len(self.dev_data)\n",
    "        elif split == 'test':\n",
    "            self._target = self.test_data\n",
    "            self._target_size = len(self.test_data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"the primary entry point method for PyTorch datasets\n",
    "        \n",
    "        Args:\n",
    "            index (int): the index to the data point \n",
    "        Returns:\n",
    "            a dictionary holding the data point's:\n",
    "                features (x_data)\n",
    "                label (y_target)\n",
    "                feature length (x_length)\n",
    "        \"\"\"\n",
    "        row = self._target[index]\n",
    "        \n",
    "        x_data = row[0]\n",
    "        y_target = [0] * len(self.label_to_index.keys())\n",
    "        if self._target != self.test_data:\n",
    "            y_target[self.label_to_index[row[1]]] = 1\n",
    "        \n",
    "        return {'x_data': torch.Tensor(x_data), \n",
    "                'y_target': torch.Tensor(y_target)}\n",
    "\n",
    "    def get_num_batches(self, batch_size):\n",
    "        \"\"\"Given a batch size, return the number of batches in the dataset\n",
    "        \n",
    "        Args:\n",
    "            batch_size (int)\n",
    "        Returns:\n",
    "            number of batches in the dataset\n",
    "        \"\"\"\n",
    "        return len(self) // batch_size\n",
    "\n",
    "    \n",
    "def generate_batches(dataset, batch_size, shuffle=True,\n",
    "                     drop_last=True, device=device): \n",
    "    \"\"\"\n",
    "    A generator function which wraps the PyTorch DataLoader. It will \n",
    "      ensure each tensor is on the write device location.\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        \n",
    "        \n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, batch_size=1):\n",
    "        super(MultiLayerPerceptron, self).__init__()\n",
    "                \n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x_in):\n",
    "        out = F.relu(self.fc1(x_in))\n",
    "        \n",
    "        out = self.dropout(out)\n",
    "        out = F.relu(self.fc2(out))\n",
    "        \n",
    "        out = self.dropout(out)\n",
    "        out = F.relu(self.fc3(out))\n",
    "        \n",
    "        out = F.log_softmax(out, dim=1)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data dict len:  23410\n"
     ]
    }
   ],
   "source": [
    "def create_dummy_bias(data):\n",
    "    for sample in data:\n",
    "        sample[0].append(1)\n",
    "    return data \n",
    "\n",
    "train_data, dev_data, test_data, data_type, label_to_index = load_data(['propername'])\n",
    "\n",
    "train_data = create_dummy_bias(train_data)\n",
    "dev_data = create_dummy_bias(dev_data)\n",
    "test_data = create_dummy_bias(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23411\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset(train_data, dev_data, test_data, label_to_index)\n",
    "\n",
    "input_dim = len(train_data[0][0])\n",
    "hidden_dim = 200\n",
    "output_dim = len(label_to_index)\n",
    "print(input_dim)\n",
    "\n",
    "classifier = MultiLayerPerceptron(input_dim, hidden_dim, output_dim, label_to_index)\n",
    "classifier.to(device)\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=8e-3, weight_decay=1e-3)\n",
    "\n",
    "def compute_accuracy(y_pred, y_target):\n",
    "    _, y_pred_indices = y_pred.max(dim=1)\n",
    "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "    return (n_correct / len(y_pred_indices)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss : 0.7573476718819661\n",
      "Training Acc:  71.5695652173913\n",
      "Development Loss:  0.4462601006031036\n",
      "Development Acc:  83.88\n",
      "Training Loss : 0.46788396589134046\n",
      "Training Acc:  82.98695652173913\n",
      "Development Loss:  0.4340768575668335\n",
      "Development Acc:  84.4\n",
      "Training Loss : 0.44488297726797016\n",
      "Training Acc:  84.09130434782607\n",
      "Development Loss:  0.45395978093147277\n",
      "Development Acc:  84.08\n",
      "Training Loss : 0.42980527165143384\n",
      "Training Acc:  84.58695652173913\n",
      "Development Loss:  0.4471429228782654\n",
      "Development Acc:  84.04\n",
      "Training Loss : 0.416516477647035\n",
      "Training Acc:  84.90434782608696\n",
      "Development Loss:  0.4067946016788483\n",
      "Development Acc:  84.91999999999999\n",
      "Training Loss : 0.4025510277437127\n",
      "Training Acc:  85.6782608695652\n",
      "Development Loss:  0.40787228345870974\n",
      "Development Acc:  84.76\n",
      "Training Loss : 0.39552190575910645\n",
      "Training Acc:  85.8173913043478\n",
      "Development Loss:  0.4079636216163635\n",
      "Development Acc:  84.80000000000001\n",
      "Training Loss : 0.39618508776892797\n",
      "Training Acc:  86.02173913043474\n",
      "Development Loss:  0.4140989601612091\n",
      "Development Acc:  85.24\n",
      "Training Loss : 0.3844010195006495\n",
      "Training Acc:  86.20434782608694\n",
      "Development Loss:  0.41351592540740967\n",
      "Development Acc:  84.52\n"
     ]
    }
   ],
   "source": [
    "#TODO:\n",
    "#HYPERPARAMETER SELECTION ON NETWORK WITH FEW LAYERS\n",
    "#THEN RUN TESTS ON DEEPER NETWORK\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    dataset.set_split('train')\n",
    "    batch_generator = generate_batches(dataset, batch_size=500)\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    classifier.train()\n",
    "        \n",
    "    for batch_index, batch_dict in enumerate(batch_generator):\n",
    "        # the training routine is these 5 steps:\n",
    "\n",
    "        # --------------------------------------\n",
    "        # step 1. zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # step 2. compute output\n",
    "        inputs, labels = batch_dict['x_data'], batch_dict['y_target']\n",
    "        y_pred = classifier(inputs)\n",
    "\n",
    "        # step 3. compute the loss\n",
    "        loss = loss_func(y_pred, torch.max(labels ,1)[1])\n",
    "        loss_t = loss.item()\n",
    "        running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "        # step 4. use loss to produce gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # step 5. use optimizer to take gradient step\n",
    "        optimizer.step()\n",
    "        # -----------------------------------------\n",
    "        # compute the accuracy\n",
    "        acc_t = compute_accuracy(y_pred, torch.max(labels, 1)[1])\n",
    "        running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "    print(\"Training Loss :\", running_loss)\n",
    "    print(\"Training Acc: \", running_acc)\n",
    "    \n",
    "    # setup: batch generator, set loss and acc to 0; set eval mode on\n",
    "    dataset.set_split('dev')\n",
    "    batch_generator = generate_batches(dataset, batch_size=500)\n",
    "    \n",
    "    running_loss = 0.\n",
    "    running_acc = 0.\n",
    "    classifier.eval()\n",
    "        \n",
    "    for batch_index, batch_dict in enumerate(batch_generator):\n",
    "\n",
    "        # step 2. compute output\n",
    "        inputs, labels = batch_dict['x_data'], batch_dict['y_target']\n",
    "        y_pred = classifier(inputs)\n",
    "\n",
    "        # step 3. compute the loss\n",
    "        loss = loss_func(y_pred, torch.max(labels,1)[1])\n",
    "        loss_t = loss.item()\n",
    "        running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "        \n",
    "        conf_mat = confusion_matrix(torch.max(labels,1)[1], y_pred, \n",
    "            labels=np.sort(np.unique(torch.max(labels,1)[1])))\n",
    "        print(conf_mat)\n",
    "        print(np.sort(np.unique(torch.max(labels,1)[1])))\n",
    "\n",
    "        # compute the accuracy\n",
    "        acc_t = compute_accuracy(y_pred, torch.max(labels, 1)[1])\n",
    "        running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "        \n",
    "    print(\"Development Loss: \", running_loss)\n",
    "    print(\"Development Acc: \", running_acc)\n",
    "    if (running_acc > 87.0):\n",
    "        break\n",
    "        \n",
    "#test predictions\n",
    "classifier.eval()\n",
    "dataset.set_split('test')\n",
    "\n",
    "batch_generator = generate_batches(dataset, batch_size=dataset.test_size, shuffle=False)\n",
    "for batch_index, batch_dict in enumerate(batch_generator):\n",
    "    y_pred = classifier(batch_dict['x_data'])\n",
    "    label_pred = [dataset.index_to_label[torch.max(y,0)[1].item()] for y in y_pred]\n",
    "\n",
    "df = pd.DataFrame(list(zip(range(len(label_pred)), label_pred)), \n",
    "               columns =['id', 'type'])\n",
    "df.to_csv(\"mlp_propername_test_predictions.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
